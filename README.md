# ğŸ¬ Movie Rental Apriori Pipeline

## Movie Rental Patterns Analysis System

Automated data mining pipeline using Apriori algorithm to discover co-rental patterns in a movie rental store, orchestrated with Apache Airflow.

---

## ğŸ“‹ Project Description

This project implements a complete data analysis pipeline to discover which movies tend to be rented together by customers. It uses the Apriori algorithm to find frequent itemsets and generate association rules that can be used for:

- **Recommendation systems**: Suggest movies based on current rentals
- **Marketing strategies**: Create promotional bundles of related movies
- **Inventory organization**: Place related movies near each other
- **Trend analysis**: Understand customer tastes and preferences

---

## ğŸ¯ Business Scenario

**Context**: We are the data team of a movie rental store.

**Problem**: Every day new rental transactions are generated, and we need to:
1. Identify which movies are frequently rented together
2. Discover customer behavior patterns
3. Generate automatic recommendations

**Solution**: An automated pipeline that:
- Processes daily transactions
- Executes the Apriori algorithm
- Generates reports with actionable insights

---

## ğŸ“Š Dataset

### Movies in Catalog
The dataset contains rental transactions with the following movies:

**Sci-Fi/Thriller Category:**
- Inception
- Interstellar
- The Dark Knight
- The Martian

**Crime/Drama Category:**
- The Godfather
- Goodfellas
- Scarface

**Action/Tarantino Category:**
- Pulp Fiction
- Kill Bill
- Django Unchained

**Superhero Category:**
- Joker
- The Batman

### Data Format
```csv
TransactionID,Items
1,"Inception,The Dark Knight,Interstellar"
2,"The Godfather,Goodfellas"
3,"Inception,Interstellar"
...
```

### Characteristics
- **40 sample transactions**
- **12 unique movies** in catalog
- **2-3 movies per transaction** on average
- Simulated data with realistic patterns

---

## ğŸ—ï¸ Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw CSV       â”‚
â”‚  (data.csv)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Task 1:       â”‚
â”‚   Load Data     â”‚ â”€â”€â–º loaded_transactions.json
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Task 2:       â”‚
â”‚   Clean Data    â”‚ â”€â”€â–º cleaned_transactions.json
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      cleaning_stats.json
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Task 3:       â”‚
â”‚   Run Apriori   â”‚ â”€â”€â–º frequent_itemsets.json
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      association_rules.json
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Task 4:       â”‚
â”‚ Generate Report â”‚ â”€â”€â–º analysis_report.txt
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      frequent_itemsets.csv
                         association_rules.csv
                         summary_statistics.json
```

---

## ğŸ”§ Requirements and Installation

### Prerequisites
- Python 3.8 or higher
- Apache Airflow 2.0 or higher
- pip (Python package manager)

### Installation

1. **Clone the repository**
```bash
git clone <your-repository>
cd movie_rental_apriori
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install apache-airflow
pip install pandas
```

4. **Configure Airflow**
```bash
# Initialize Airflow database
airflow db init

# Create admin user
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin
```

5. **Set AIRFLOW_HOME environment variable**
```bash
export AIRFLOW_HOME=~/airflow  # On Windows: set AIRFLOW_HOME=%USERPROFILE%\airflow
```

6. **Copy DAG to Airflow directory**
```bash
cp dags/apriori_pipeline_dag.py $AIRFLOW_HOME/dags/
```

---

## ğŸš€ How to Run

### Option 1: Run with Airflow (Recommended)

1. **Start Airflow webserver**
```bash
airflow webserver --port 8080
```

2. **In another terminal, start the scheduler**
```bash
airflow scheduler
```

3. **Access web interface**
- Open browser at: `http://localhost:8080`
- Login: `admin` / `admin`

4. **Execute the DAG**
- Search for DAG: `movie_rental_apriori_pipeline`
- Activate toggle to enable it
- Click "Trigger DAG" to run manually

### Option 2: Run Individual Scripts (For Testing)

```bash
# From project root directory

# 1. Load data
python scripts/load_data.py

# 2. Clean data
python scripts/clean_data.py

# 3. Run Apriori
python scripts/apriori.py

# 4. Generate reports
python scripts/generate_report.py
```

---

## ğŸ“ Project Structure

```
movie_rental_apriori/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ apriori_pipeline_dag.py       # Airflow DAG
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ data.csv                  # Raw transaction data
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ loaded_transactions.json   # Loaded data
â”‚   â”‚   â”œâ”€â”€ cleaned_transactions.json  # Clean data
â”‚   â”‚   â””â”€â”€ cleaning_stats.json        # Cleaning statistics
â”‚   â””â”€â”€ results/
â”‚       â”œâ”€â”€ frequent_itemsets.json     # Frequent itemsets
â”‚       â”œâ”€â”€ association_rules.json     # Association rules
â”‚       â”œâ”€â”€ analysis_report.txt        # Readable report
â”‚       â”œâ”€â”€ frequent_itemsets.csv      # Itemsets in CSV
â”‚       â”œâ”€â”€ association_rules.csv      # Rules in CSV
â”‚       â””â”€â”€ summary_statistics.json    # Summary statistics
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ load_data.py                  # Loading script
â”‚   â”œâ”€â”€ clean_data.py                 # Cleaning script
â”‚   â”œâ”€â”€ apriori.py                    # Apriori implementation
â”‚   â””â”€â”€ generate_report.py            # Report generator
â”‚
â””â”€â”€ README.md                          # This file
```

---

## ğŸ§® Apriori Algorithm - Explanation

### What is Apriori?

Apriori is a data mining algorithm designed to find **frequent itemsets** (sets of items that appear together frequently) and generate **association rules** (co-occurrence patterns).

### Step-by-Step Operation

#### 1. Find Frequent Individual Items (L1)
```
Scan transactions and count each movie
Example:
- Inception: appears in 15 transactions (support = 15/40 = 37.5%)
- The Dark Knight: 12 transactions (30%)
...
Filter by minimum support (15%)
```

#### 2. Generate Pair Candidates (C2)
```
Combine frequent L1 items to create pairs
Example:
- {Inception, The Dark Knight}
- {Inception, Interstellar}
- {The Godfather, Goodfellas}
...
```

#### 3. Prune Using Apriori Property
```
Principle: If an itemset is frequent, 
           all its subsets must also be frequent

Example: If {A, B, C} is a candidate,
         verify that {A,B}, {A,C}, {B,C} are frequent
         If any is not, eliminate {A,B,C}
```

#### 4. Calculate Candidate Support (L2)
```
Scan transactions and count pairs
Filter by minimum support
```

#### 5. Repeat for Triples, Quadruples, etc.
```
Continue until no more frequent itemsets exist
```

#### 6. Generate Association Rules
```
For each frequent itemset of size â‰¥ 2:
  Divide into antecedent â†’ consequent
  
Example: {Inception, Interstellar, The Dark Knight}
  â†’ {Inception, Interstellar} => {The Dark Knight}
  â†’ {Inception} => {Interstellar, The Dark Knight}
  ... all possible combinations
  
Calculate metrics:
  - Support: % of transactions containing complete itemset
  - Confidence: % of times consequent appears given antecedent
  - Lift: How much more likely is consequent given antecedent
```

### Metrics Explained

**Support**: How frequent is this itemset?
```
Support({A,B}) = Transactions with A and B / Total transactions
Example: 6/40 = 0.15 (15%)
```

**Confidence**: If someone rents A, how likely are they to rent B?
```
Confidence(Aâ†’B) = Support({A,B}) / Support({A})
Example: 0.15 / 0.30 = 0.50 (50%)
```

**Lift**: Is the rule better than chance?
```
Lift(Aâ†’B) = Confidence(Aâ†’B) / Support({B})
Lift > 1: Positive correlation (useful!)
Lift = 1: Independent
Lift < 1: Negative correlation
```

### Configurable Parameters

In `scripts/apriori.py`:
```python
apriori = AprioriAlgorithm(
    min_support=0.15,      # 15% - Adjust as needed
    min_confidence=0.5,    # 50% - More reliable rules
    min_lift=1.0           # Only positive correlations
)
```

---

## ğŸ“ˆ Results Interpretation

### Frequent Itemset Example
```
Itemset: [Inception, Interstellar]
Support: 0.35 (35%)

Interpretation:
35% of customers who rent movies 
rent these two together.
```

### Association Rule Example
```
Rule: [Inception] => [Interstellar]
Support: 0.35
Confidence: 0.78
Lift: 2.1

Interpretation:
- If a customer rents Inception, there's a 78% probability 
  they will also rent Interstellar
- This combination is 2.1 times more likely than if 
  they were independent
- Action: Recommend Interstellar to those who rent Inception
```

---

## ğŸ“Š Results Visualization

Results can be visualized in several files:

### 1. Text Report (`analysis_report.txt`)
- Executive summary
- Frequent itemsets by size
- Top 20 rules ordered by lift
- Key insights

### 2. Itemsets CSV (`frequent_itemsets.csv`)
- Easy to open in Excel/Google Sheets
- Useful for additional analysis

### 3. Rules CSV (`association_rules.csv`)
- All metrics in tabular format
- Filterable and sortable

### 4. JSON Statistics (`summary_statistics.json`)
- Aggregate metrics
- Useful for dashboards

---

## ğŸ“ DAG Tasks Description

### Task 1: Load Data
**File**: `scripts/load_data.py`

**Responsibility**:
- Read raw CSV file
- Validate data format
- Convert to JSON for processing

**Input**: `data/raw/data.csv`

**Output**: `data/processed/loaded_transactions.json`

**XCom**: Publishes number of loaded transactions

---

### Task 2: Clean Data
**File**: `scripts/clean_data.py`

**Responsibility**:
- Separate items by commas
- Remove whitespace
- Filter empty transactions
- Generate descriptive statistics

**Input**: `data/processed/loaded_transactions.json`

**Output**: 
- `data/processed/cleaned_transactions.json`
- `data/processed/cleaning_stats.json`

**XCom**: Publishes number of clean transactions and unique movies

---

### Task 3: Run Apriori
**File**: `scripts/apriori.py`

**Responsibility**:
- Complete Apriori algorithm implementation
- Find frequent itemsets
- Generate association rules
- Calculate metrics (support, confidence, lift)

**Input**: `data/processed/cleaned_transactions.json`

**Output**:
- `data/results/frequent_itemsets.json`
- `data/results/association_rules.json`

**XCom**: Publishes number of itemsets and rules found

**Algorithm**:
1. Generate size-1 itemsets
2. For each size k:
   - Generate size-k candidates
   - Prune using Apriori property
   - Calculate support and filter
3. Generate association rules
4. Calculate confidence and lift

---

### Task 4: Generate Report
**File**: `scripts/generate_report.py`

**Responsibility**:
- Create readable text report
- Export to CSV for Excel
- Generate summary statistics
- Produce actionable insights

**Input**:
- `data/results/frequent_itemsets.json`
- `data/results/association_rules.json`

**Output**:
- `data/results/analysis_report.txt`
- `data/results/frequent_itemsets.csv`
- `data/results/association_rules.csv`
- `data/results/summary_statistics.json`

---

## ğŸ” Results Verification

### 1. Verify DAG executed correctly
In Airflow UI, all tasks should be green (success).

### 2. Review main report
```bash
cat data/results/analysis_report.txt
```

### 3. Verify generated files
```bash
ls -la data/results/
```

You should see:
- `frequent_itemsets.json`
- `association_rules.json`
- `analysis_report.txt`
- `frequent_itemsets.csv`
- `association_rules.csv`
- `summary_statistics.json`

### 4. Open CSVs in Excel/Google Sheets
CSV files are ideal for visual analysis and filtering.

---

## âš ï¸ Limitations

1. **Small Dataset**: Only 40 transactions for demonstration
2. **Simulated Data**: Not real transactions
3. **Limited Catalog**: Only 12 movies
4. **No Persistence**: Each execution overwrites previous results
5. **No Data Validation**: Doesn't handle edge cases or corrupted data
6. **Fixed Parameters**: Min support, confidence and lift don't adjust dynamically

---

## ğŸš€ Future Improvements

### Functionality
- [ ] Support for multiple CSV files (incremental processing)
- [ ] Day-to-day trend comparison
- [ ] Seasonal pattern detection
- [ ] Alert system for anomalous patterns
- [ ] REST API to query results

### Visualization
- [ ] Interactive dashboard with Plotly/Dash
- [ ] Association network graphs
- [ ] Co-occurrence heatmaps
- [ ] 3D lift visualization

### Scalability
- [ ] Database integration (PostgreSQL)
- [ ] Distributed processing with Spark
- [ ] Frequent itemsets caching
- [ ] Calculation parallelization

### Intelligence
- [ ] Automatic parameter adjustment
- [ ] Movie clustering by patterns
- [ ] Future trend prediction
- [ ] Personalized recommendations per customer

### DevOps
- [ ] Unit and integration tests
- [ ] CI/CD with GitHub Actions
- [ ] Pipeline Dockerization
- [ ] Monitoring with Prometheus/Grafana

---

## ğŸ¤ Contributions

This is an individual academic project, but suggestions are welcome:

1. Fork the repository
2. Create branch for your feature (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is for academic and educational purposes.

---

## ğŸ‘¤ Author

**Diego De Gante**
- Final Project - Data Mining
- [Universidad PolitÃ©cnica de YucatÃ¡n]
- [degantediego@gmail.com]

---

## ğŸ“š References

- Agrawal, R., & Srikant, R. (1994). Fast algorithms for mining association rules
- Apache Airflow Documentation: https://airflow.apache.org/
- Python Documentation: https://docs.python.org/

---

## â“ FAQ

**Q: Why use Airflow for such a small project?**

A: Although the dataset is small, Airflow demonstrates pipeline orchestration skills that are critical in production environments. It's practice for larger projects.

**Q: Can I use my own data?**

A: Yes! Replace `data/raw/data.csv` with your own file following the same format.

**Q: How do I change Apriori parameters?**

A: Edit the lines in `scripts/apriori.py` and in the DAG where `AprioriAlgorithm` is initialized.

**Q: Is the algorithm efficient for large data?**

A: This implementation is optimized for clarity, not scale. For millions of transactions, consider Spark MLlib or optimized implementations.

**Q: Why not use mlxtend?**

A: The project requires manual implementation to demonstrate understanding of the algorithm.

---

## Thank you for reviewing this project!

If you have questions or suggestions, don't hesitate to contact me.
